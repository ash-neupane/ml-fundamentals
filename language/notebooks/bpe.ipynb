{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to answer\n",
    "1. Is byte-pair encoding information theory optimal? How do you know if something is optimal? In other words, does an uniform distribution across the vocabulary tokens represent an uniform distribution across a language? Does this question even make sense? If not, what is the right way to think about how well the tokenization represents the prevalence of individual tokens?\n",
    "\n",
    "2. How do modern language models pick the vocabulary size? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Auto-Regressive Language Models are classifers. They produce a probability distribution of the prediction of what comes next on the space defined by the vocabulary. A naive way is to use characters as this vocabulary. The drawback of this is that a lot of time and resources is spent on learning how to build words. It's a lot better to give the language models the words we already know and use. The drawback with using words however is that it limits the model's ability to learn and use new words. From an information theory perspective, it also doesn't represent the importance of these words in our day to day usage. Tokenization with Byte Pair Encoding is a sweet spot between characters and words where we build our vocabulary with chunks of characters based on how common they are in our language. This gives the language model the most common words so it doesnt need to learn them from scratch alongside the ability to learn and use new words or just words that are not as common. Tokens are chunks of commonly occuring sequences of characters in language. Tokenization is the process of translating text into sequences of tokens and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research History\n",
    "\n",
    "* First defined by Phil Gage for Data Compression in http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM \n",
    "* Byte Pair Encoding was introduced in the context of language modeling for Neural Machine Translation of Rare Words with Subword Units: https://arxiv.org/pdf/1508.07909\n",
    "    * a\n",
    "* And operationalized into large language models in GPT-2: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "    * With slight modifications: \n",
    "\n",
    "* More recently, there was research into how larger models need larger vocabulary. Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies https://arxiv.org/abs/2407.13623\n",
    "    *  a\n",
    "\n",
    "\n",
    "## Other Relevant Papers\n",
    "* Language Models spend a lot of early layer compute translating tokens to meaningful units, and late layers reversing this: https://arxiv.org/pdf/2406.09519.\n",
    "* (biomedical domain llm trained with a specialized tokenizer) Specialized domains benefit from domain-specific tokenization: https://link.springer.com/chapter/10.1007/978-3-031-41682-8_2 \n",
    "* Byte Pair Encoding is sub-optimal?: https://arxiv.org/abs/2004.03720 \n",
    "* Section III C. of Large Language Models: A Survey: https://arxiv.org/pdf/2402.06196 : Brief intros to bpe, wpe, and spe. No substance.\n",
    "* Tokenization is More Than Compression: https://arxiv.org/abs/2402.18376\n",
    "* (tokenization introduces a sampling bias) Understanding and Mitigating Tokenization Bias in Language Models: https://arxiv.org/abs/2406.16829\n",
    "* Pre-GPT4 research on how tokenization affects neural machine translation. How Much Does Tokenization Affect Neural Machine Translation?: https://arxiv.org/abs/1812.08621 \n",
    "* (Nepali language tokenization??) Can Perplexity Predict Fine-Tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali: https://arxiv.org/pdf/2404.18071v1 \n",
    "* Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization: https://arxiv.org/pdf/2405.17067\n",
    "* (Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations?) Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs: https://arxiv.org/abs/2406.20086\n",
    "* Tries to do away with tokenization altogether. MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers https://arxiv.org/abs/2305.07185 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some known effects of Tokenization\n",
    "\n",
    "* LLMs can't spell words perfectly.\n",
    "* Basic arithmetic is difficult in a hilarious way.\n",
    "* Simple string processing (especially character level) like counting the number of characters, reversing words.\n",
    "* Worse ability on non-english languages - because tokenization is primarily done on english language\n",
    "* etc.... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Playground\n",
    "https://tiktokenizer.vercel.app/ \n",
    "\n",
    "Tokenization is kind of arbitrary. Spaces sometimes change the same word from being one token to multiple tokens. Numbers are tokenized in a way that looks completely random to us. Non-english languages are broken up to more tokens to represent the same sentence -> this is part of why non-english performance is worse. GPT-2 tokenizer struggled with coding partly because each whitespace was a separate token and that was wasteful, the newer models fix this. GPT-4 tokenizer drops token count for same information from GPT-2, cause the vocabulary size is double. And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "Along the lines of Andrej Karpathy's tutorial on GPT Tokenizer: https://www.youtube.com/watch?v=zduSFxRajkE&t=1430s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode\n",
    "Cant use all of unicode code points (is what they call characters), because \n",
    "1. it's too large = 150k \n",
    "2. and it keeps changing. \n",
    "\n",
    "Solution: encodings like\n",
    "* utf-8 (1-4 bytes, most common, backwards compatible with the older ascii encoding)\n",
    "* utf-16\n",
    "* utf-32 (fixed length but a bit wasteful)\n",
    "More on unicode: https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "\n",
    "\n",
    "If we use utf-8 as the entire vocabulary (character level - ish), there's only 256 elements. Makes context lengths really long when we represent \n",
    "english language in it. Transformers (specifically the Attention layers) scale quadratic with context length. We can do better. \n",
    "Solution: Byte Pair Encoding Algorithm. \n",
    "\n",
    "Side: Feeding raw bytes makes language models end to end, and potentially mitigates some of the quirks of tokenization? But it doesnt work as far as we know. MEGABYTE studies if this can be done - need to modify architecture, hasnt been verified/reproduced/studied by enough people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h:  104\n",
      "µ:  181\n",
      "hello world bytes in utf-8: [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]\n",
      "hello world bytes in utf-32 (wasteful!): [255, 254, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 0, 0, 119, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 108, 0, 0, 0, 100, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# unicode standard\n",
    "print(\"h: \", ord(\"h\"))\n",
    "print(\"µ: \", ord('µ')) \n",
    "\n",
    "print(\"hello world bytes in utf-8:\", list(\"hello world\".encode(\"utf-8\")))\n",
    "print(\"hello world bytes in utf-32 (wasteful!):\", list(\"hello world\".encode(\"utf-32\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding \n",
    "(AKA Digram coding)\n",
    "Wikipedia page has good explanation: https://en.wikipedia.org/wiki/Byte_pair_encoding \n",
    "\n",
    "Iteratively:\n",
    "* Find the pair of tokens that occur most frequently\n",
    "* Replace that pair with a single token that we replace their occurence with it. \n",
    "* Until there are no pairs of bytes that occur more than once. In practice: the stopping condition is when a vocabulary of desired size is obtained. Vocabulary size is typically a hyperparameter.\n",
    "\n",
    "Increases vocabulary size, but compresses sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 ˜ßµœπœ∫¥¨©∂ø ^ _ . º ' ` ˛ ÿ Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial 'tokens'). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] This algorithmic approach have been extended from spoken language to sign language in recent years.[9]All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256.. The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for tokenization because it has low computational overhead and remains consistent and reliable. Original algorithm: The original algorithm operates by iteratively replacing the most common contiguous sequences of characters in a target text with unused 'placeholder' bytes. The iteration ends when no sequences can be found, leaving the target text effectively compressed. Decompression can be performed by reversing this process, querying known placeholder terms against their corresponding denoted sequence, using a lookup table. In the original paper, this lookup table is encoded and stored alongside the compressed text. Example: Suppose the data to be encoded is aaabdaaabac The byte pair 'aa' occurs most often, so it will be replaced by a byte that is not used in the data, such as 'Z'. Now there is the following data and replacement table: ZabdZabac, Z=aa, Then the process is repeated with byte pair 'ab', replacing it with 'Y': ZYdZYac, Y=ab, Z=aa. The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing 'ZY' with 'X': XdXac, X=ZY, Y=ab, Z=aa. This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once. To decompress the data, simply perform the replacements in the reverse order.\n",
      "2965\n",
      "---------------------------------\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 203, 156, 195, 159, 194, 181, 197, 147, 207, 128, 197, 147, 226, 136, 171, 194, 165, 194, 168, 194, 169, 226, 136, 130, 195, 184, 32, 94, 32, 95, 32, 46, 32, 194, 186, 32, 39, 32, 96, 32, 203, 155, 32, 195, 191, 32, 66, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 91, 49, 93, 91, 50, 93, 32, 40, 97, 108, 115, 111, 32, 107, 110, 111, 119, 110, 32, 97, 115, 32, 100, 105, 103, 114, 97, 109, 32, 99, 111, 100, 105, 110, 103, 41, 91, 51, 93, 32, 105, 115, 32, 97, 110, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 44, 32, 102, 105, 114, 115, 116, 32, 100, 101, 115, 99, 114, 105, 98, 101, 100, 32, 105, 110, 32, 49, 57, 57, 52, 32, 98, 121, 32, 80, 104, 105, 108, 105, 112, 32, 71, 97, 103, 101, 32, 102, 111, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 115, 116, 114, 105, 110, 103, 115, 32, 111, 102, 32, 116, 101, 120, 116, 32, 105, 110, 116, 111, 32, 116, 97, 98, 117, 108, 97, 114, 32, 102, 111, 114, 109, 32, 102, 111, 114, 32, 117, 115, 101, 32, 105, 110, 32, 100, 111, 119, 110, 115, 116, 114, 101, 97, 109, 32, 109, 111, 100, 101, 108, 105, 110, 103, 46, 91, 52, 93, 32, 73, 116, 115, 32, 109, 111, 100, 105, 102, 105, 99, 97, 116, 105, 111, 110, 32, 105, 115, 32, 110, 111, 116, 97, 98, 108, 101, 32, 97, 115, 32, 116, 104, 101, 32, 108, 97, 114, 103, 101, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 109, 111, 100, 101, 108, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 119, 105, 116, 104, 32, 97, 110, 32, 97, 98, 105, 108, 105, 116, 121, 32, 116, 111, 32, 99, 111, 109, 98, 105, 110, 101, 32, 98, 111, 116, 104, 32, 116, 111, 107, 101, 110, 115, 32, 116, 104, 97, 116, 32, 101, 110, 99, 111, 100, 101, 32, 115, 105, 110, 103, 108, 101, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 40, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 115, 105, 110, 103, 108, 101, 32, 100, 105, 103, 105, 116, 115, 32, 111, 114, 32, 115, 105, 110, 103, 108, 101, 32, 112, 117, 110, 99, 116, 117, 97, 116, 105, 111, 110, 32, 109, 97, 114, 107, 115, 41, 32, 97, 110, 100, 32, 116, 104, 111, 115, 101, 32, 116, 104, 97, 116, 32, 101, 110, 99, 111, 100, 101, 32, 119, 104, 111, 108, 101, 32, 119, 111, 114, 100, 115, 32, 40, 101, 118, 101, 110, 32, 116, 104, 101, 32, 108, 111, 110, 103, 101, 115, 116, 32, 99, 111, 109, 112, 111, 117, 110, 100, 32, 119, 111, 114, 100, 115, 41, 46, 91, 53, 93, 91, 54, 93, 91, 55, 93, 32, 84, 104, 105, 115, 32, 109, 111, 100, 105, 102, 105, 99, 97, 116, 105, 111, 110, 44, 32, 105, 110, 32, 116, 104, 101, 32, 102, 105, 114, 115, 116, 32, 115, 116, 101, 112, 44, 32, 97, 115, 115, 117, 109, 101, 115, 32, 97, 108, 108, 32, 117, 110, 105, 113, 117, 101, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 116, 111, 32, 98, 101, 32, 97, 110, 32, 105, 110, 105, 116, 105, 97, 108, 32, 115, 101, 116, 32, 111, 102, 32, 49, 45, 99, 104, 97, 114, 97, 99, 116, 101, 114, 32, 108, 111, 110, 103, 32, 110, 45, 103, 114, 97, 109, 115, 32, 40, 105, 46, 101, 46, 32, 105, 110, 105, 116, 105, 97, 108, 32, 39, 116, 111, 107, 101, 110, 115, 39, 41, 46, 32, 84, 104, 101, 110, 44, 32, 115, 117, 99, 99, 101, 115, 115, 105, 118, 101, 108, 121, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 102, 114, 101, 113, 117, 101, 110, 116, 32, 112, 97, 105, 114, 32, 111, 102, 32, 97, 100, 106, 97, 99, 101, 110, 116, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 105, 115, 32, 109, 101, 114, 103, 101, 100, 32, 105, 110, 116, 111, 32, 97, 32, 110, 101, 119, 44, 32, 50, 45, 99, 104, 97, 114, 97, 99, 116, 101, 114, 32, 108, 111, 110, 103, 32, 110, 45, 103, 114, 97, 109, 32, 97, 110, 100, 32, 97, 108, 108, 32, 105, 110, 115, 116, 97, 110, 99, 101, 115, 32, 111, 102, 32, 116, 104, 101, 32, 112, 97, 105, 114, 32, 97, 114, 101, 32, 114, 101, 112, 108, 97, 99, 101, 100, 32, 98, 121, 32, 116, 104, 105, 115, 32, 110, 101, 119, 32, 116, 111, 107, 101, 110, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 114, 101, 112, 101, 97, 116, 101, 100, 32, 117, 110, 116, 105, 108, 32, 97, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 32, 111, 102, 32, 112, 114, 101, 115, 99, 114, 105, 98, 101, 100, 32, 115, 105, 122, 101, 32, 105, 115, 32, 111, 98, 116, 97, 105, 110, 101, 100, 46, 32, 78, 111, 116, 101, 32, 116, 104, 97, 116, 32, 110, 101, 119, 32, 119, 111, 114, 100, 115, 32, 99, 97, 110, 32, 97, 108, 119, 97, 121, 115, 32, 98, 101, 32, 99, 111, 110, 115, 116, 114, 117, 99, 116, 101, 100, 32, 102, 114, 111, 109, 32, 102, 105, 110, 97, 108, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 32, 116, 111, 107, 101, 110, 115, 32, 97, 110, 100, 32, 105, 110, 105, 116, 105, 97, 108, 45, 115, 101, 116, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 46, 91, 56, 93, 32, 84, 104, 105, 115, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 105, 99, 32, 97, 112, 112, 114, 111, 97, 99, 104, 32, 104, 97, 118, 101, 32, 98, 101, 101, 110, 32, 101, 120, 116, 101, 110, 100, 101, 100, 32, 102, 114, 111, 109, 32, 115, 112, 111, 107, 101, 110, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 116, 111, 32, 115, 105, 103, 110, 32, 108, 97, 110, 103, 117, 97, 103, 101, 32, 105, 110, 32, 114, 101, 99, 101, 110, 116, 32, 121, 101, 97, 114, 115, 46, 91, 57, 93, 65, 108, 108, 32, 116, 104, 101, 32, 117, 110, 105, 113, 117, 101, 32, 116, 111, 107, 101, 110, 115, 32, 102, 111, 117, 110, 100, 32, 105, 110, 32, 97, 32, 99, 111, 114, 112, 117, 115, 32, 97, 114, 101, 32, 108, 105, 115, 116, 101, 100, 32, 105, 110, 32, 97, 32, 116, 111, 107, 101, 110, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 44, 32, 116, 104, 101, 32, 115, 105, 122, 101, 32, 111, 102, 32, 119, 104, 105, 99, 104, 44, 32, 105, 110, 32, 116, 104, 101, 32, 99, 97, 115, 101, 32, 111, 102, 32, 71, 80, 84, 45, 51, 46, 53, 32, 97, 110, 100, 32, 71, 80, 84, 45, 52, 44, 32, 105, 115, 32, 49, 48, 48, 50, 53, 54, 46, 46, 32, 84, 104, 101, 32, 100, 105, 102, 102, 101, 114, 101, 110, 99, 101, 32, 98, 101, 116, 119, 101, 101, 110, 32, 116, 104, 101, 32, 109, 111, 100, 105, 102, 105, 101, 100, 32, 97, 110, 100, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 105, 115, 32, 116, 104, 97, 116, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 100, 111, 101, 115, 32, 110, 111, 116, 32, 109, 101, 114, 103, 101, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 102, 114, 101, 113, 117, 101, 110, 116, 32, 112, 97, 105, 114, 32, 111, 102, 32, 98, 121, 116, 101, 115, 32, 111, 102, 32, 100, 97, 116, 97, 44, 32, 98, 117, 116, 32, 114, 101, 112, 108, 97, 99, 101, 115, 32, 116, 104, 101, 109, 32, 98, 121, 32, 97, 32, 110, 101, 119, 32, 98, 121, 116, 101, 32, 116, 104, 97, 116, 32, 119, 97, 115, 32, 110, 111, 116, 32, 99, 111, 110, 116, 97, 105, 110, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 105, 110, 105, 116, 105, 97, 108, 32, 100, 97, 116, 97, 115, 101, 116, 46, 32, 65, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 32, 111, 102, 32, 116, 104, 101, 32, 114, 101, 112, 108, 97, 99, 101, 109, 101, 110, 116, 115, 32, 105, 115, 32, 114, 101, 113, 117, 105, 114, 101, 100, 32, 116, 111, 32, 114, 101, 98, 117, 105, 108, 100, 32, 116, 104, 101, 32, 105, 110, 105, 116, 105, 97, 108, 32, 100, 97, 116, 97, 115, 101, 116, 46, 32, 84, 104, 101, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 105, 115, 32, 101, 102, 102, 101, 99, 116, 105, 118, 101, 32, 102, 111, 114, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 98, 101, 99, 97, 117, 115, 101, 32, 105, 116, 32, 104, 97, 115, 32, 108, 111, 119, 32, 99, 111, 109, 112, 117, 116, 97, 116, 105, 111, 110, 97, 108, 32, 111, 118, 101, 114, 104, 101, 97, 100, 32, 97, 110, 100, 32, 114, 101, 109, 97, 105, 110, 115, 32, 99, 111, 110, 115, 105, 115, 116, 101, 110, 116, 32, 97, 110, 100, 32, 114, 101, 108, 105, 97, 98, 108, 101, 46, 32, 79, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 58, 32, 84, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 111, 112, 101, 114, 97, 116, 101, 115, 32, 98, 121, 32, 105, 116, 101, 114, 97, 116, 105, 118, 101, 108, 121, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 99, 111, 109, 109, 111, 110, 32, 99, 111, 110, 116, 105, 103, 117, 111, 117, 115, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 111, 102, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 32, 105, 110, 32, 97, 32, 116, 97, 114, 103, 101, 116, 32, 116, 101, 120, 116, 32, 119, 105, 116, 104, 32, 117, 110, 117, 115, 101, 100, 32, 39, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 39, 32, 98, 121, 116, 101, 115, 46, 32, 84, 104, 101, 32, 105, 116, 101, 114, 97, 116, 105, 111, 110, 32, 101, 110, 100, 115, 32, 119, 104, 101, 110, 32, 110, 111, 32, 115, 101, 113, 117, 101, 110, 99, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 102, 111, 117, 110, 100, 44, 32, 108, 101, 97, 118, 105, 110, 103, 32, 116, 104, 101, 32, 116, 97, 114, 103, 101, 116, 32, 116, 101, 120, 116, 32, 101, 102, 102, 101, 99, 116, 105, 118, 101, 108, 121, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 46, 32, 68, 101, 99, 111, 109, 112, 114, 101, 115, 115, 105, 111, 110, 32, 99, 97, 110, 32, 98, 101, 32, 112, 101, 114, 102, 111, 114, 109, 101, 100, 32, 98, 121, 32, 114, 101, 118, 101, 114, 115, 105, 110, 103, 32, 116, 104, 105, 115, 32, 112, 114, 111, 99, 101, 115, 115, 44, 32, 113, 117, 101, 114, 121, 105, 110, 103, 32, 107, 110, 111, 119, 110, 32, 112, 108, 97, 99, 101, 104, 111, 108, 100, 101, 114, 32, 116, 101, 114, 109, 115, 32, 97, 103, 97, 105, 110, 115, 116, 32, 116, 104, 101, 105, 114, 32, 99, 111, 114, 114, 101, 115, 112, 111, 110, 100, 105, 110, 103, 32, 100, 101, 110, 111, 116, 101, 100, 32, 115, 101, 113, 117, 101, 110, 99, 101, 44, 32, 117, 115, 105, 110, 103, 32, 97, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 46, 32, 73, 110, 32, 116, 104, 101, 32, 111, 114, 105, 103, 105, 110, 97, 108, 32, 112, 97, 112, 101, 114, 44, 32, 116, 104, 105, 115, 32, 108, 111, 111, 107, 117, 112, 32, 116, 97, 98, 108, 101, 32, 105, 115, 32, 101, 110, 99, 111, 100, 101, 100, 32, 97, 110, 100, 32, 115, 116, 111, 114, 101, 100, 32, 97, 108, 111, 110, 103, 115, 105, 100, 101, 32, 116, 104, 101, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 32, 116, 101, 120, 116, 46, 32, 69, 120, 97, 109, 112, 108, 101, 58, 32, 83, 117, 112, 112, 111, 115, 101, 32, 116, 104, 101, 32, 100, 97, 116, 97, 32, 116, 111, 32, 98, 101, 32, 101, 110, 99, 111, 100, 101, 100, 32, 105, 115, 32, 97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99, 32, 84, 104, 101, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 39, 97, 97, 39, 32, 111, 99, 99, 117, 114, 115, 32, 109, 111, 115, 116, 32, 111, 102, 116, 101, 110, 44, 32, 115, 111, 32, 105, 116, 32, 119, 105, 108, 108, 32, 98, 101, 32, 114, 101, 112, 108, 97, 99, 101, 100, 32, 98, 121, 32, 97, 32, 98, 121, 116, 101, 32, 116, 104, 97, 116, 32, 105, 115, 32, 110, 111, 116, 32, 117, 115, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 100, 97, 116, 97, 44, 32, 115, 117, 99, 104, 32, 97, 115, 32, 39, 90, 39, 46, 32, 78, 111, 119, 32, 116, 104, 101, 114, 101, 32, 105, 115, 32, 116, 104, 101, 32, 102, 111, 108, 108, 111, 119, 105, 110, 103, 32, 100, 97, 116, 97, 32, 97, 110, 100, 32, 114, 101, 112, 108, 97, 99, 101, 109, 101, 110, 116, 32, 116, 97, 98, 108, 101, 58, 32, 90, 97, 98, 100, 90, 97, 98, 97, 99, 44, 32, 90, 61, 97, 97, 44, 32, 84, 104, 101, 110, 32, 116, 104, 101, 32, 112, 114, 111, 99, 101, 115, 115, 32, 105, 115, 32, 114, 101, 112, 101, 97, 116, 101, 100, 32, 119, 105, 116, 104, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 39, 97, 98, 39, 44, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 105, 116, 32, 119, 105, 116, 104, 32, 39, 89, 39, 58, 32, 90, 89, 100, 90, 89, 97, 99, 44, 32, 89, 61, 97, 98, 44, 32, 90, 61, 97, 97, 46, 32, 84, 104, 101, 32, 111, 110, 108, 121, 32, 108, 105, 116, 101, 114, 97, 108, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 108, 101, 102, 116, 32, 111, 99, 99, 117, 114, 115, 32, 111, 110, 108, 121, 32, 111, 110, 99, 101, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 109, 105, 103, 104, 116, 32, 115, 116, 111, 112, 32, 104, 101, 114, 101, 46, 32, 65, 108, 116, 101, 114, 110, 97, 116, 105, 118, 101, 108, 121, 44, 32, 116, 104, 101, 32, 112, 114, 111, 99, 101, 115, 115, 32, 99, 111, 117, 108, 100, 32, 99, 111, 110, 116, 105, 110, 117, 101, 32, 119, 105, 116, 104, 32, 114, 101, 99, 117, 114, 115, 105, 118, 101, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 44, 32, 114, 101, 112, 108, 97, 99, 105, 110, 103, 32, 39, 90, 89, 39, 32, 119, 105, 116, 104, 32, 39, 88, 39, 58, 32, 88, 100, 88, 97, 99, 44, 32, 88, 61, 90, 89, 44, 32, 89, 61, 97, 98, 44, 32, 90, 61, 97, 97, 46, 32, 84, 104, 105, 115, 32, 100, 97, 116, 97, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 32, 99, 111, 109, 112, 114, 101, 115, 115, 101, 100, 32, 102, 117, 114, 116, 104, 101, 114, 32, 98, 121, 32, 98, 121, 116, 101, 32, 112, 97, 105, 114, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 98, 101, 99, 97, 117, 115, 101, 32, 116, 104, 101, 114, 101, 32, 97, 114, 101, 32, 110, 111, 32, 112, 97, 105, 114, 115, 32, 111, 102, 32, 98, 121, 116, 101, 115, 32, 116, 104, 97, 116, 32, 111, 99, 99, 117, 114, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 111, 110, 99, 101, 46, 32, 84, 111, 32, 100, 101, 99, 111, 109, 112, 114, 101, 115, 115, 32, 116, 104, 101, 32, 100, 97, 116, 97, 44, 32, 115, 105, 109, 112, 108, 121, 32, 112, 101, 114, 102, 111, 114, 109, 32, 116, 104, 101, 32, 114, 101, 112, 108, 97, 99, 101, 109, 101, 110, 116, 115, 32, 105, 110, 32, 116, 104, 101, 32, 114, 101, 118, 101, 114, 115, 101, 32, 111, 114, 100, 101, 114, 46]\n",
      "3055\n"
     ]
    }
   ],
   "source": [
    "# example text: wikipedia's bpe article with some random unicode at staart\n",
    "global_text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 ˜ßµœπœ∫¥¨©∂ø ^ _ . º ' ` ˛ \\xFF Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial 'tokens'). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] This algorithmic approach have been extended from spoken language to sign language in recent years.[9]All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256.. The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for tokenization because it has low computational overhead and remains consistent and reliable. Original algorithm: The original algorithm operates by iteratively replacing the most common contiguous sequences of characters in a target text with unused 'placeholder' bytes. The iteration ends when no sequences can be found, leaving the target text effectively compressed. Decompression can be performed by reversing this process, querying known placeholder terms against their corresponding denoted sequence, using a lookup table. In the original paper, this lookup table is encoded and stored alongside the compressed text. Example: Suppose the data to be encoded is aaabdaaabac The byte pair 'aa' occurs most often, so it will be replaced by a byte that is not used in the data, such as 'Z'. Now there is the following data and replacement table: ZabdZabac, Z=aa, Then the process is repeated with byte pair 'ab', replacing it with 'Y': ZYdZYac, Y=ab, Z=aa. The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing 'ZY' with 'X': XdXac, X=ZY, Y=ab, Z=aa. This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once. To decompress the data, simply perform the replacements in the reverse order.\"\n",
    "global_tokens = list(map(int, global_text.encode(\"utf-8\"))) \n",
    "\n",
    "print(global_text)\n",
    "print(len(global_text))\n",
    "print(\"---------------------------------\")\n",
    "print(global_tokens)\n",
    "print(len(global_tokens))\n",
    "# tokens are longer than text cause the special characters need more than one byte. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(89, (101, 32)), (68, (32, 116)), (62, (115, 32)), (59, (116, 104)), (53, (105, 110)), (44, (104, 101)), (44, (32, 97)), (40, (101, 110)), (39, (114, 101)), (38, (116, 101)), (38, (116, 32)), (38, (32, 105)), (37, (100, 32)), (35, (110, 32)), (31, (101, 114)), (30, (32, 98)), (29, (32, 111)), (28, (110, 103)), (27, (99, 111)), (27, (44, 32)), (26, (111, 114)), (26, (105, 116)), (26, (97, 116)), (26, (32, 99)), (25, (97, 108)), (24, (101, 100)), (24, (97, 99)), (23, (105, 115)), (23, (101, 115)), (22, (111, 110)), (22, (97, 110)), (21, (114, 32)), (21, (99, 101)), (20, (116, 97)), (19, (115, 101)), (19, (108, 97)), (19, (97, 114)), (19, (32, 115)), (19, (32, 114)), (19, (32, 112)), (18, (116, 105)), (18, (110, 99)), (18, (108, 32)), (18, (97, 98)), (18, (46, 32)), (17, (116, 111)), (17, (115, 116)), (17, (104, 97)), (17, (98, 121)), (16, (121, 32)), (16, (110, 100)), (16, (32, 100)), (15, (240, 159)), (15, (114, 105)), (15, (111, 100)), (15, (103, 32)), (15, (32, 108)), (15, (32, 102)), (14, (115, 105)), (14, (114, 115)), (14, (114, 97)), (14, (110, 116)), (14, (105, 114)), (14, (100, 105)), (14, (100, 101)), (14, (97, 105)), (14, (32, 119)), (14, (32, 109)), (13, (112, 108)), (13, (111, 102)), (13, (108, 101)), (13, (98, 101)), (13, (32, 101)), (13, (32, 84)), (12, (111, 107)), (12, (111, 32)), (12, (109, 32)), (12, (102, 32)), (12, (101, 112)), (12, (99, 97)), (12, (84, 104)), (12, (32, 110)), (11, (121, 116)), (11, (118, 101)), (11, (112, 97)), (11, (111, 109)), (11, (109, 111)), (11, (99, 116)), (11, (97, 32)), (10, (115, 115)), (10, (112, 114)), (10, (110, 115)), (10, (110, 111)), (10, (105, 103)), (10, (103, 101)), (10, (99, 104)), (9, (117, 101)), (9, (113, 117)), (9, (111, 99)), (9, (110, 105)), (9, (109, 112)), (9, (108, 111)), (9, (107, 101)), (9, (104, 105)), (9, (104, 32)), (9, (102, 111)), (9, (100, 97)), (9, (97, 115)), (9, (32, 39)), (8, (119, 105)), (8, (117, 115)), (8, (117, 110)), (8, (111, 116)), (8, (110, 97)), (8, (101, 99)), (8, (97, 97)), (7, (226, 128)), (7, (159, 135)), (7, (159, 133)), (7, (110, 101)), (7, (109, 101)), (7, (108, 121)), (7, (108, 103)), (7, (105, 111)), (7, (104, 109)), (7, (103, 111)), (7, (101, 116)), (7, (101, 108)), (7, (32, 117)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (114, 111)), (6, (112, 101)), (6, (111, 119)), (6, (111, 115)), (6, (108, 105)), (6, (105, 118)), (6, (105, 97)), (6, (103, 105)), (6, (102, 105)), (6, (101, 113)), (6, (101, 97)), (6, (98, 117)), (6, (98, 108)), (5, (120, 116)), (5, (119, 32)), (5, (117, 114)), (5, (117, 108)), (5, (114, 103)), (5, (112, 32)), (5, (111, 117)), (5, (108, 108)), (5, (105, 108)), (5, (101, 120)), (5, (101, 109)), (5, (101, 46)), (5, (97, 109)), (5, (97, 103)), (5, (93, 32)), (5, (61, 97)), (5, (58, 32)), (5, (32, 90)), (4, (117, 112)), (4, (117, 97)), (4, (116, 115)), (4, (114, 121)), (4, (114, 109)), (4, (114, 100)), (4, (112, 111)), (4, (111, 108)), (4, (108, 100)), (4, (105, 122)), (4, (105, 102)), (4, (105, 99)), (4, (104, 111)), (4, (103, 117)), (4, (102, 114)), (4, (101, 119)), (4, (100, 115)), (4, (99, 117)), (4, (99, 99)), (4, (97, 44)), (4, (90, 89)), (4, (46, 91)), (4, (39, 32)), (4, (32, 40)), (3, (122, 101)), (3, (119, 111)), (3, (119, 110)), (3, (119, 104)), (3, (118, 111)), (3, (117, 99)), (3, (116, 114)), (3, (116, 46)), (3, (115, 117)), (3, (115, 46)), (3, (112, 117)), (3, (111, 111)), (3, (110, 44)), (3, (107, 117)), (3, (103, 114)), (3, (103, 108)), (3, (102, 102)), (3, (102, 101)), (3, (101, 118)), (3, (101, 102)), (3, (99, 105)), (3, (99, 44)), (3, (93, 91)), (3, (90, 61)), (3, (32, 240)), (3, (32, 118)), (3, (32, 104)), (3, (32, 71)), (3, (32, 49)), (2, (226, 136)), (2, (197, 147)), (2, (169, 226)), (2, (121, 44)), (2, (119, 97)), (2, (117, 116)), (2, (117, 105)), (2, (115, 112)), (2, (115, 111)), (2, (115, 99)), (2, (115, 41)), (2, (114, 102)), (2, (112, 112)), (2, (111, 112)), (2, (110, 117)), (2, (110, 108)), (2, (110, 45)), (2, (109, 115)), (2, (109, 105)), (2, (109, 97)), (2, (107, 110)), (2, (105, 113)), (2, (105, 98)), (2, (103, 115)), (2, (102, 116)), (2, (101, 104)), (2, (101, 101)), (2, (101, 58)), (2, (101, 44)), (2, (100, 111)), (2, (100, 90)), (2, (100, 46)), (2, (99, 114)), (2, (99, 32)), (2, (98, 105)), (2, (98, 100)), (2, (98, 97)), (2, (98, 44)), (2, (97, 118)), (2, (97, 117)), (2, (97, 112)), (2, (97, 100)), (2, (97, 46)), (2, (90, 97)), (2, (89, 61)), (2, (89, 39)), (2, (84, 45)), (2, (80, 84)), (2, (78, 111)), (2, (71, 80)), (2, (65, 108)), (2, (45, 103)), (2, (45, 99)), (2, (41, 46)), (2, (40, 105)), (2, (39, 97)), (2, (39, 90)), (2, (39, 58)), (2, (33, 32)), (2, (32, 203)), (2, (32, 107)), (2, (32, 89)), (2, (32, 88)), (2, (32, 78)), (2, (32, 73)), (2, (32, 65)), (1, (239, 188)), (1, (207, 128)), (1, (203, 156)), (1, (203, 155)), (1, (195, 191)), (1, (195, 184)), (1, (195, 159)), (1, (194, 186)), (1, (194, 181)), (1, (194, 169)), (1, (194, 168)), (1, (194, 165)), (1, (191, 32)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (186, 32)), (1, (184, 32)), (1, (181, 239)), (1, (181, 197)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (171, 194)), (1, (170, 33)), (1, (168, 226)), (1, (168, 194)), (1, (165, 194)), (1, (164, 240)), (1, (159, 194)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (156, 195)), (1, (155, 32)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (147, 240)), (1, (147, 226)), (1, (147, 207)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (136, 171)), (1, (136, 130)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (130, 195)), (1, (128, 197)), (1, (128, 189)), (1, (122, 97)), (1, (121, 115)), (1, (121, 105)), (1, (121, 101)), (1, (120, 97)), (1, (119, 101)), (1, (119, 44)), (1, (118, 105)), (1, (117, 111)), (1, (117, 109)), (1, (117, 100)), (1, (116, 121)), (1, (116, 119)), (1, (116, 117)), (1, (115, 44)), (1, (115, 39)), (1, (114, 117)), (1, (114, 116)), (1, (114, 114)), (1, (114, 112)), (1, (114, 110)), (1, (114, 107)), (1, (114, 104)), (1, (114, 46)), (1, (114, 44)), (1, (114, 39)), (1, (112, 44)), (1, (111, 118)), (1, (111, 101)), (1, (111, 98)), (1, (111, 97)), (1, (110, 110)), (1, (110, 46)), (1, (109, 109)), (1, (109, 98)), (1, (109, 58)), (1, (109, 44)), (1, (108, 119)), (1, (108, 117)), (1, (108, 116)), (1, (108, 115)), (1, (108, 45)), (1, (107, 115)), (1, (106, 97)), (1, (105, 112)), (1, (105, 109)), (1, (105, 101)), (1, (105, 100)), (1, (105, 46)), (1, (104, 116)), (1, (104, 44)), (1, (103, 110)), (1, (103, 104)), (1, (103, 97)), (1, (103, 91)), (1, (103, 46)), (1, (103, 44)), (1, (103, 41)), (1, (102, 117)), (1, (101, 105)), (1, (101, 98)), (1, (100, 106)), (1, (100, 88)), (1, (100, 44)), (1, (99, 108)), (1, (98, 116)), (1, (98, 111)), (1, (98, 39)), (1, (97, 121)), (1, (97, 39)), (1, (96, 32)), (1, (95, 32)), (1, (94, 32)), (1, (93, 65)), (1, (91, 57)), (1, (91, 56)), (1, (91, 55)), (1, (91, 54)), (1, (91, 53)), (1, (91, 52)), (1, (91, 51)), (1, (91, 50)), (1, (91, 49)), (1, (90, 39)), (1, (89, 100)), (1, (89, 97)), (1, (89, 44)), (1, (88, 100)), (1, (88, 97)), (1, (88, 61)), (1, (88, 39)), (1, (84, 111)), (1, (83, 117)), (1, (80, 104)), (1, (79, 114)), (1, (73, 116)), (1, (73, 110)), (1, (71, 97)), (1, (69, 120)), (1, (68, 101)), (1, (66, 121)), (1, (65, 32)), (1, (61, 90)), (1, (57, 93)), (1, (57, 57)), (1, (57, 52)), (1, (56, 93)), (1, (55, 93)), (1, (54, 93)), (1, (54, 46)), (1, (53, 93)), (1, (53, 54)), (1, (53, 32)), (1, (52, 93)), (1, (52, 44)), (1, (52, 32)), (1, (51, 93)), (1, (51, 46)), (1, (50, 93)), (1, (50, 53)), (1, (50, 45)), (1, (49, 93)), (1, (49, 57)), (1, (49, 48)), (1, (49, 45)), (1, (48, 50)), (1, (48, 48)), (1, (46, 101)), (1, (46, 53)), (1, (46, 46)), (1, (45, 115)), (1, (45, 52)), (1, (45, 51)), (1, (41, 91)), (1, (41, 32)), (1, (40, 101)), (1, (40, 97)), (1, (39, 116)), (1, (39, 112)), (1, (39, 89)), (1, (39, 88)), (1, (39, 46)), (1, (39, 44)), (1, (39, 41)), (1, (32, 195)), (1, (32, 194)), (1, (32, 121)), (1, (32, 113)), (1, (32, 96)), (1, (32, 95)), (1, (32, 94)), (1, (32, 83)), (1, (32, 80)), (1, (32, 79)), (1, (32, 69)), (1, (32, 68)), (1, (32, 66)), (1, (32, 50)), (1, (32, 46))]\n",
      "[(89, ('e', ' ')), (68, (' ', 't')), (62, ('s', ' ')), (59, ('t', 'h')), (53, ('i', 'n')), (44, ('h', 'e')), (44, (' ', 'a')), (40, ('e', 'n')), (39, ('r', 'e')), (38, ('t', 'e')), (38, ('t', ' ')), (38, (' ', 'i')), (37, ('d', ' ')), (35, ('n', ' ')), (31, ('e', 'r')), (30, (' ', 'b')), (29, (' ', 'o')), (28, ('n', 'g')), (27, ('c', 'o')), (27, (',', ' ')), (26, ('o', 'r')), (26, ('i', 't')), (26, ('a', 't')), (26, (' ', 'c')), (25, ('a', 'l')), (24, ('e', 'd')), (24, ('a', 'c')), (23, ('i', 's')), (23, ('e', 's')), (22, ('o', 'n')), (22, ('a', 'n')), (21, ('r', ' ')), (21, ('c', 'e')), (20, ('t', 'a')), (19, ('s', 'e')), (19, ('l', 'a')), (19, ('a', 'r')), (19, (' ', 's')), (19, (' ', 'r')), (19, (' ', 'p')), (18, ('t', 'i')), (18, ('n', 'c')), (18, ('l', ' ')), (18, ('a', 'b')), (18, ('.', ' ')), (17, ('t', 'o')), (17, ('s', 't')), (17, ('h', 'a')), (17, ('b', 'y')), (16, ('y', ' ')), (16, ('n', 'd')), (16, (' ', 'd')), (15, ('ð', '\\x9f')), (15, ('r', 'i')), (15, ('o', 'd')), (15, ('g', ' ')), (15, (' ', 'l')), (15, (' ', 'f')), (14, ('s', 'i')), (14, ('r', 's')), (14, ('r', 'a')), (14, ('n', 't')), (14, ('i', 'r')), (14, ('d', 'i')), (14, ('d', 'e')), (14, ('a', 'i')), (14, (' ', 'w')), (14, (' ', 'm')), (13, ('p', 'l')), (13, ('o', 'f')), (13, ('l', 'e')), (13, ('b', 'e')), (13, (' ', 'e')), (13, (' ', 'T')), (12, ('o', 'k')), (12, ('o', ' ')), (12, ('m', ' ')), (12, ('f', ' ')), (12, ('e', 'p')), (12, ('c', 'a')), (12, ('T', 'h')), (12, (' ', 'n')), (11, ('y', 't')), (11, ('v', 'e')), (11, ('p', 'a')), (11, ('o', 'm')), (11, ('m', 'o')), (11, ('c', 't')), (11, ('a', ' ')), (10, ('s', 's')), (10, ('p', 'r')), (10, ('n', 's')), (10, ('n', 'o')), (10, ('i', 'g')), (10, ('g', 'e')), (10, ('c', 'h')), (9, ('u', 'e')), (9, ('q', 'u')), (9, ('o', 'c')), (9, ('n', 'i')), (9, ('m', 'p')), (9, ('l', 'o')), (9, ('k', 'e')), (9, ('h', 'i')), (9, ('h', ' ')), (9, ('f', 'o')), (9, ('d', 'a')), (9, ('a', 's')), (9, (' ', \"'\")), (8, ('w', 'i')), (8, ('u', 's')), (8, ('u', 'n')), (8, ('o', 't')), (8, ('n', 'a')), (8, ('e', 'c')), (8, ('a', 'a')), (7, ('â', '\\x80')), (7, ('\\x9f', '\\x87')), (7, ('\\x9f', '\\x85')), (7, ('n', 'e')), (7, ('m', 'e')), (7, ('l', 'y')), (7, ('l', 'g')), (7, ('i', 'o')), (7, ('h', 'm')), (7, ('g', 'o')), (7, ('e', 't')), (7, ('e', 'l')), (7, (' ', 'u')), (6, ('ï', '½')), (6, ('\\x8c', 'ð')), (6, ('\\x80', '\\x8c')), (6, ('r', 'o')), (6, ('p', 'e')), (6, ('o', 'w')), (6, ('o', 's')), (6, ('l', 'i')), (6, ('i', 'v')), (6, ('i', 'a')), (6, ('g', 'i')), (6, ('f', 'i')), (6, ('e', 'q')), (6, ('e', 'a')), (6, ('b', 'u')), (6, ('b', 'l')), (5, ('x', 't')), (5, ('w', ' ')), (5, ('u', 'r')), (5, ('u', 'l')), (5, ('r', 'g')), (5, ('p', ' ')), (5, ('o', 'u')), (5, ('l', 'l')), (5, ('i', 'l')), (5, ('e', 'x')), (5, ('e', 'm')), (5, ('e', '.')), (5, ('a', 'm')), (5, ('a', 'g')), (5, (']', ' ')), (5, ('=', 'a')), (5, (':', ' ')), (5, (' ', 'Z')), (4, ('u', 'p')), (4, ('u', 'a')), (4, ('t', 's')), (4, ('r', 'y')), (4, ('r', 'm')), (4, ('r', 'd')), (4, ('p', 'o')), (4, ('o', 'l')), (4, ('l', 'd')), (4, ('i', 'z')), (4, ('i', 'f')), (4, ('i', 'c')), (4, ('h', 'o')), (4, ('g', 'u')), (4, ('f', 'r')), (4, ('e', 'w')), (4, ('d', 's')), (4, ('c', 'u')), (4, ('c', 'c')), (4, ('a', ',')), (4, ('Z', 'Y')), (4, ('.', '[')), (4, (\"'\", ' ')), (4, (' ', '(')), (3, ('z', 'e')), (3, ('w', 'o')), (3, ('w', 'n')), (3, ('w', 'h')), (3, ('v', 'o')), (3, ('u', 'c')), (3, ('t', 'r')), (3, ('t', '.')), (3, ('s', 'u')), (3, ('s', '.')), (3, ('p', 'u')), (3, ('o', 'o')), (3, ('n', ',')), (3, ('k', 'u')), (3, ('g', 'r')), (3, ('g', 'l')), (3, ('f', 'f')), (3, ('f', 'e')), (3, ('e', 'v')), (3, ('e', 'f')), (3, ('c', 'i')), (3, ('c', ',')), (3, (']', '[')), (3, ('Z', '=')), (3, (' ', 'ð')), (3, (' ', 'v')), (3, (' ', 'h')), (3, (' ', 'G')), (3, (' ', '1')), (2, ('â', '\\x88')), (2, ('Å', '\\x93')), (2, ('©', 'â')), (2, ('y', ',')), (2, ('w', 'a')), (2, ('u', 't')), (2, ('u', 'i')), (2, ('s', 'p')), (2, ('s', 'o')), (2, ('s', 'c')), (2, ('s', ')')), (2, ('r', 'f')), (2, ('p', 'p')), (2, ('o', 'p')), (2, ('n', 'u')), (2, ('n', 'l')), (2, ('n', '-')), (2, ('m', 's')), (2, ('m', 'i')), (2, ('m', 'a')), (2, ('k', 'n')), (2, ('i', 'q')), (2, ('i', 'b')), (2, ('g', 's')), (2, ('f', 't')), (2, ('e', 'h')), (2, ('e', 'e')), (2, ('e', ':')), (2, ('e', ',')), (2, ('d', 'o')), (2, ('d', 'Z')), (2, ('d', '.')), (2, ('c', 'r')), (2, ('c', ' ')), (2, ('b', 'i')), (2, ('b', 'd')), (2, ('b', 'a')), (2, ('b', ',')), (2, ('a', 'v')), (2, ('a', 'u')), (2, ('a', 'p')), (2, ('a', 'd')), (2, ('a', '.')), (2, ('Z', 'a')), (2, ('Y', '=')), (2, ('Y', \"'\")), (2, ('T', '-')), (2, ('P', 'T')), (2, ('N', 'o')), (2, ('G', 'P')), (2, ('A', 'l')), (2, ('-', 'g')), (2, ('-', 'c')), (2, (')', '.')), (2, ('(', 'i')), (2, (\"'\", 'a')), (2, (\"'\", 'Z')), (2, (\"'\", ':')), (2, ('!', ' ')), (2, (' ', 'Ë')), (2, (' ', 'k')), (2, (' ', 'Y')), (2, (' ', 'X')), (2, (' ', 'N')), (2, (' ', 'I')), (2, (' ', 'A')), (1, ('ï', '¼')), (1, ('Ï', '\\x80')), (1, ('Ë', '\\x9c')), (1, ('Ë', '\\x9b')), (1, ('Ã', '¿')), (1, ('Ã', '¸')), (1, ('Ã', '\\x9f')), (1, ('Â', 'º')), (1, ('Â', 'µ')), (1, ('Â', '©')), (1, ('Â', '¨')), (1, ('Â', '¥')), (1, ('¿', ' ')), (1, ('½', '\\x8f')), (1, ('½', '\\x8e')), (1, ('½', '\\x89')), (1, ('½', '\\x85')), (1, ('½', '\\x84')), (1, ('½', '\\x83')), (1, ('½', ' ')), (1, ('¼', 'µ')), (1, ('º', 'â')), (1, ('º', ' ')), (1, ('¸', ' ')), (1, ('µ', 'ï')), (1, ('µ', 'Å')), (1, ('´', 'â')), (1, ('³', 'â')), (1, ('®', 'â')), (1, ('«', 'Â')), (1, ('ª', '!')), (1, ('¨', 'â')), (1, ('¨', 'Â')), (1, ('¥', 'Â')), (1, ('¤', 'ð')), (1, ('\\x9f', 'Â')), (1, ('\\x9f', '\\x98')), (1, ('\\x9e', 'ð')), (1, ('\\x9d', 'ð')), (1, ('\\x9c', 'Ã')), (1, ('\\x9b', ' ')), (1, ('\\x98', 'ð')), (1, ('\\x98', '\\x84')), (1, ('\\x94', 'â')), (1, ('\\x93', 'ð')), (1, ('\\x93', 'â')), (1, ('\\x93', 'Ï')), (1, ('\\x92', 'ð')), (1, ('\\x8f', 'ï')), (1, ('\\x8e', 'ï')), (1, ('\\x89', 'ï')), (1, ('\\x88', '«')), (1, ('\\x88', '\\x82')), (1, ('\\x87', 'º')), (1, ('\\x87', '´')), (1, ('\\x87', '³')), (1, ('\\x87', '®')), (1, ('\\x87', 'ª')), (1, ('\\x87', '©')), (1, ('\\x87', '¨')), (1, ('\\x85', '¤')), (1, ('\\x85', '\\x9e')), (1, ('\\x85', '\\x9d')), (1, ('\\x85', '\\x98')), (1, ('\\x85', '\\x94')), (1, ('\\x85', '\\x93')), (1, ('\\x85', '\\x92')), (1, ('\\x85', '!')), (1, ('\\x84', 'ï')), (1, ('\\x84', ' ')), (1, ('\\x83', 'ï')), (1, ('\\x82', 'Ã')), (1, ('\\x80', 'Å')), (1, ('\\x80', '½')), (1, ('z', 'a')), (1, ('y', 's')), (1, ('y', 'i')), (1, ('y', 'e')), (1, ('x', 'a')), (1, ('w', 'e')), (1, ('w', ',')), (1, ('v', 'i')), (1, ('u', 'o')), (1, ('u', 'm')), (1, ('u', 'd')), (1, ('t', 'y')), (1, ('t', 'w')), (1, ('t', 'u')), (1, ('s', ',')), (1, ('s', \"'\")), (1, ('r', 'u')), (1, ('r', 't')), (1, ('r', 'r')), (1, ('r', 'p')), (1, ('r', 'n')), (1, ('r', 'k')), (1, ('r', 'h')), (1, ('r', '.')), (1, ('r', ',')), (1, ('r', \"'\")), (1, ('p', ',')), (1, ('o', 'v')), (1, ('o', 'e')), (1, ('o', 'b')), (1, ('o', 'a')), (1, ('n', 'n')), (1, ('n', '.')), (1, ('m', 'm')), (1, ('m', 'b')), (1, ('m', ':')), (1, ('m', ',')), (1, ('l', 'w')), (1, ('l', 'u')), (1, ('l', 't')), (1, ('l', 's')), (1, ('l', '-')), (1, ('k', 's')), (1, ('j', 'a')), (1, ('i', 'p')), (1, ('i', 'm')), (1, ('i', 'e')), (1, ('i', 'd')), (1, ('i', '.')), (1, ('h', 't')), (1, ('h', ',')), (1, ('g', 'n')), (1, ('g', 'h')), (1, ('g', 'a')), (1, ('g', '[')), (1, ('g', '.')), (1, ('g', ',')), (1, ('g', ')')), (1, ('f', 'u')), (1, ('e', 'i')), (1, ('e', 'b')), (1, ('d', 'j')), (1, ('d', 'X')), (1, ('d', ',')), (1, ('c', 'l')), (1, ('b', 't')), (1, ('b', 'o')), (1, ('b', \"'\")), (1, ('a', 'y')), (1, ('a', \"'\")), (1, ('`', ' ')), (1, ('_', ' ')), (1, ('^', ' ')), (1, (']', 'A')), (1, ('[', '9')), (1, ('[', '8')), (1, ('[', '7')), (1, ('[', '6')), (1, ('[', '5')), (1, ('[', '4')), (1, ('[', '3')), (1, ('[', '2')), (1, ('[', '1')), (1, ('Z', \"'\")), (1, ('Y', 'd')), (1, ('Y', 'a')), (1, ('Y', ',')), (1, ('X', 'd')), (1, ('X', 'a')), (1, ('X', '=')), (1, ('X', \"'\")), (1, ('T', 'o')), (1, ('S', 'u')), (1, ('P', 'h')), (1, ('O', 'r')), (1, ('I', 't')), (1, ('I', 'n')), (1, ('G', 'a')), (1, ('E', 'x')), (1, ('D', 'e')), (1, ('B', 'y')), (1, ('A', ' ')), (1, ('=', 'Z')), (1, ('9', ']')), (1, ('9', '9')), (1, ('9', '4')), (1, ('8', ']')), (1, ('7', ']')), (1, ('6', ']')), (1, ('6', '.')), (1, ('5', ']')), (1, ('5', '6')), (1, ('5', ' ')), (1, ('4', ']')), (1, ('4', ',')), (1, ('4', ' ')), (1, ('3', ']')), (1, ('3', '.')), (1, ('2', ']')), (1, ('2', '5')), (1, ('2', '-')), (1, ('1', ']')), (1, ('1', '9')), (1, ('1', '0')), (1, ('1', '-')), (1, ('0', '2')), (1, ('0', '0')), (1, ('.', 'e')), (1, ('.', '5')), (1, ('.', '.')), (1, ('-', 's')), (1, ('-', '4')), (1, ('-', '3')), (1, (')', '[')), (1, (')', ' ')), (1, ('(', 'e')), (1, ('(', 'a')), (1, (\"'\", 't')), (1, (\"'\", 'p')), (1, (\"'\", 'Y')), (1, (\"'\", 'X')), (1, (\"'\", '.')), (1, (\"'\", ',')), (1, (\"'\", ')')), (1, (' ', 'Ã')), (1, (' ', 'Â')), (1, (' ', 'y')), (1, (' ', 'q')), (1, (' ', '`')), (1, (' ', '_')), (1, (' ', '^')), (1, (' ', 'S')), (1, (' ', 'P')), (1, (' ', 'O')), (1, (' ', 'E')), (1, (' ', 'D')), (1, (' ', 'B')), (1, (' ', '2')), (1, (' ', '.'))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\"Find the most common pair\"\"\"\n",
    "    counts = {}\n",
    "    for ch_pair in zip(ids[:-1], ids[1:]):\n",
    "        counts[ch_pair] = counts.get(ch_pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "sorted_counts = sorted(((v,k) for k, v in stats.items()), reverse=True)\n",
    "print(sorted_counts)\n",
    "\n",
    "# fully decoding with .decode like below to convert to strings doesnt work when the encoding is variable length.\n",
    "# sorted_counts_ch = [(v, bytes(list(k)).decode(\"utf-8\")) for v, k in sorted_counts]\n",
    "sorted_counts_ch = [(v, (chr(k[0]), chr(k[1]))) for v, k in sorted_counts]\n",
    "print(sorted_counts_ch)\n",
    "\n",
    "# lot of words end with e or s, and start with t. h commonly follows t -> the is the most common english word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to implement this algorithm without following Andrej Karpathy. It's like a leetcode puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Frequency = namedtuple(\"Frequency\", [\"pair\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent(ids):\n",
    "    \"\"\"Given a sequence of token ids, find the most frequent\"\"\"\n",
    "    most_frequent = Frequency(pair=None, count=0)\n",
    "    counts = {}\n",
    "    for pair in zip(ids[:-1], ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "        if counts[pair] > most_frequent.count:\n",
    "            most_frequent = Frequency(pair=pair, count=counts[pair])\n",
    "    return most_frequent\n",
    "\n",
    "def merge_tokens(ids, id1, id2, id_new):\n",
    "    \"\"\"Merge tokens 'id1'+'id2' in the sequence 'ids' into 'id_new', in place.\"\"\"\n",
    "    if len(ids) < 2:\n",
    "        return\n",
    "    write_i = 0\n",
    "    read_i = 0\n",
    "    while read_i < len(ids):\n",
    "        if read_i == len(ids)-1:\n",
    "            ids[write_i] = ids[read_i]\n",
    "        elif ids[read_i] == id1 and ids[read_i+1] == id2:\n",
    "            ids[write_i] = id_new\n",
    "            read_i += 1 # skip next token because its merged\n",
    "        else:\n",
    "            ids[write_i] = ids[read_i]\n",
    "        read_i += 1\n",
    "        write_i += 1\n",
    "    ids[:] = ids[:write_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING get_most_frequent ...\n",
      "test case 0: ([], Frequency(pair=None, count=0))\n",
      "\tPASS\n",
      "test case 1: ([1], Frequency(pair=None, count=0))\n",
      "\tPASS\n",
      "test case 2: ([1, 2, 3, 4, 5], Frequency(pair=(1, 2), count=1))\n",
      "\tPASS\n",
      "test case 3: ([1, 2, 1, 2, 3, 4], Frequency(pair=(1, 2), count=2))\n",
      "\tPASS\n",
      "test case 4: ([1, 2, 3, 4, 1, 2, 3, 4, 1, 2], Frequency(pair=(1, 2), count=3))\n",
      "\tPASS\n",
      "test case 5: ([1, 1, 1, 1], Frequency(pair=(1, 1), count=3))\n",
      "\tPASS\n",
      "test case 6: ([1000000, 2000000, 1000000, 2000000], Frequency(pair=(1000000, 2000000), count=2))\n",
      "\tPASS\n",
      "test case 7: ([1, 3, 3, 2, 2, 3, 3, 3], Frequency(pair=(3, 3), count=3))\n",
      "\tPASS\n",
      "test case 8: ([1, 2, 3, 2, 3, 1], Frequency(pair=(2, 3), count=2))\n",
      "\tPASS\n",
      "test case 9: ([1, 1, 2, 2, 1, 1, 3, 3, 1, 1], Frequency(pair=(1, 1), count=3))\n",
      "\tPASS\n",
      "-----------------------------------\n",
      "10 PASSED, 0 FAILED.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_get_most_frequent(ids, expected_most_frequent):\n",
    "    most_frequent = get_most_frequent(ids)\n",
    "    assert most_frequent.pair == expected_most_frequent.pair, f\"pair mismatch: expected {expected_most_frequent.pair}, got {most_frequent.pair}\"\n",
    "    assert most_frequent.count == expected_most_frequent.count, f\"count mismatch: expected {expected_most_frequent.count}, got {most_frequent.count}\"\n",
    "\n",
    "most_frequent_test_cases = [\n",
    "    ([], Frequency(None, 0)),\n",
    "    ([1], Frequency(None, 0)),\n",
    "    ([1, 2, 3, 4, 5], Frequency((1,2), 1)),\n",
    "    ([1, 2, 1, 2, 3, 4], Frequency((1, 2), 2)),\n",
    "    ([1, 2, 3, 4, 1, 2, 3, 4, 1, 2], Frequency((1, 2), 3)),\n",
    "    ([1, 1, 1, 1], Frequency((1, 1), 3)),\n",
    "    ([1000000, 2000000, 1000000, 2000000], Frequency((1000000, 2000000), 2)),\n",
    "    ([1, 3, 3, 2, 2, 3, 3, 3], Frequency((3, 3), 3)),\n",
    "    ([1, 2, 3, 2, 3, 1], Frequency((2, 3), 2)),\n",
    "    ([1, 1, 2, 2, 1, 1, 3, 3, 1, 1], Frequency((1, 1), 3))\n",
    "]\n",
    "print(\"\\nTESTING get_most_frequent ...\")\n",
    "fail_count = 0\n",
    "for i, most_frequent_test in enumerate(most_frequent_test_cases):\n",
    "    print(f\"test case {i}: {most_frequent_test}\")\n",
    "    try:\n",
    "        test_get_most_frequent(*most_frequent_test)\n",
    "        print(\"\\tPASS\")\n",
    "    except Exception as e:\n",
    "        fail_count += 1\n",
    "        print(f\"xxxxxxx FAIL. {str(e)}\")\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"{len(most_frequent_test_cases) - fail_count} PASSED, {fail_count} FAILED.\")\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING merge_tokens...\n",
      "test case 0 [1, 2, 3, 4] --> 2+3=5 --> [1, 5, 4]\n",
      "\tPASS\n",
      "test case 1 [1, 2, 3, 2, 3, 4] --> 2+3=5 --> [1, 5, 5, 4]\n",
      "\tPASS\n",
      "test case 2 [1, 2, 3, 4] --> 1+2=5 --> [5, 3, 4]\n",
      "\tPASS\n",
      "test case 3 [1, 2, 3, 4] --> 3+4=5 --> [1, 2, 5]\n",
      "\tPASS\n",
      "test case 4 [1, 2, 4, 3] --> 2+3=5 --> [1, 2, 4, 3]\n",
      "\tPASS\n",
      "test case 5 [] --> 1+2=3 --> []\n",
      "\tPASS\n",
      "test case 6 [1] --> 1+2=3 --> [1]\n",
      "\tPASS\n",
      "test case 7 [1, 2, 1, 2, 1, 2] --> 1+2=3 --> [3, 3, 3]\n",
      "\tPASS\n",
      "test case 8 [1, 1, 1, 1] --> 1+1=2 --> [2, 2]\n",
      "\tPASS\n",
      "test case 9 [1, 2, 3, 4, 5] --> 5+6=7 --> [1, 2, 3, 4, 5]\n",
      "\tPASS\n",
      "-----------------------------------\n",
      "10 PASSED, 0 FAILED.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_merge_tokens(ids, id1, id2, id_new, expected_ids):\n",
    "    merge_tokens(ids, id1, id2, id_new)\n",
    "    assert len(ids) == len(expected_ids), f\"expected {len(expected_ids)} long, got {len(ids)} long\"\n",
    "    for i in range(len(expected_ids)):\n",
    "        # can do check list equal, looping through items makes error message more useful\n",
    "        assert ids[i] == expected_ids[i], f\"Merged tokens mismatch at index i, expected {expected_ids[i]}, got {ids[i]}\"\n",
    "\n",
    "merge_token_tests = [\n",
    "    ([1, 2, 3, 4], 2, 3, 5, [1, 5, 4]),\n",
    "    ([1, 2, 3, 2, 3, 4], 2, 3, 5, [1, 5, 5, 4]),\n",
    "    ([1, 2, 3, 4], 1, 2, 5, [5, 3, 4]),\n",
    "    ([1, 2, 3, 4], 3, 4, 5, [1, 2, 5]),\n",
    "    ([1, 2, 4, 3], 2, 3, 5, [1, 2, 4, 3]),\n",
    "    ([], 1, 2, 3, []),\n",
    "    ([1], 1, 2, 3, [1]),\n",
    "    ([1, 2, 1, 2, 1, 2], 1, 2, 3, [3, 3, 3]),\n",
    "    ([1, 1, 1, 1], 1, 1, 2, [2, 2]),\n",
    "    ([1, 2, 3, 4, 5], 5, 6, 7, [1, 2, 3, 4, 5])\n",
    "]\n",
    "print(\"\\nTESTING merge_tokens...\")\n",
    "fail_count = 0\n",
    "for i, merge_token_test in enumerate(merge_token_tests):\n",
    "    print(f\"test case {i} {merge_token_test[0]} --> {merge_token_test[1]}+{merge_token_test[2]}={merge_token_test[3]} --> {merge_token_test[4]}\")\n",
    "    try:\n",
    "        test_merge_tokens(*merge_token_test)\n",
    "        print(\"\\tPASS\")\n",
    "    except Exception as e:\n",
    "        fail_count += 1\n",
    "        print(f\"xxxxxx FAIL. {str(e)}\")\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"{len(most_frequent_test_cases) - fail_count} PASSED, {fail_count} FAILED.\")\n",
    "print(\"-----------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bpe(tokens, vocab_size, target_vocab_size, early_stop_no_repeat=False):\n",
    "    merged_tokens_map = {} # remember how the new tokens map to characters\n",
    "    def token_to_char(id):\n",
    "        if id < 256:\n",
    "            return chr(id)\n",
    "        else:\n",
    "            return merged_tokens_map[id]\n",
    "\n",
    "    while vocab_size < target_vocab_size:\n",
    "        frequent = get_most_frequent(tokens)\n",
    "        if frequent.count < 2 and early_stop_no_repeat:\n",
    "            # no more multiple occurences left\n",
    "            break\n",
    "        merge_tokens(tokens, frequent.pair[0], frequent.pair[1], vocab_size)\n",
    "        merged_tokens_map[vocab_size] = token_to_char(frequent.pair[0]) + token_to_char(frequent.pair[1])\n",
    "        vocab_size += 1\n",
    "    return merged_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global tokens compressed from 3055 to 1505 tokens, compression ratio = 2.03 \n",
      "New vocab size = 256, New tokens:  {256: 'e ', 257: 's ', 258: 'th', 259: 'in', 260: 'en', 261: 't ', 262: 'd ', 263: 're', 264: 'er', 265: 'the ', 266: ' a', 267: 'co', 268: ', ', 269: 'ac', 270: 'or', 271: 'is ', 272: 'ing', 273: 'ed ', 274: 'ta', 275: 'al', 276: ' b', 277: '. ', 278: 'to', 279: 'ti', 280: 'on', 281: 'ar', 282: ' the ', 283: 'ð\\x9f', 284: 'an', 285: 'ith', 286: 'ir', 287: 'of', 288: 'pl', 289: 'ter', 290: 'Th', 291: 'y ', 292: 'ab', 293: 'pa', 294: 'yt', 295: 'plac', 296: 'cod', 297: 'pair', 298: 'ce', 299: 'ken', 300: 'm ', 301: 'qu', 302: 'mo', 303: 'ch', 304: 'ig', 305: 'ing ', 306: 'encod', 307: 'no', 308: 'com', 309: 'da', 310: 'replac', 311: 'token', 312: 'un', 313: 'se', 314: 'st ', 315: 'yte ', 316: 'pair ', 317: 'data', 318: 'si', 319: 'ð\\x9f\\x85', 320: 'â\\x80', 321: 'ð\\x9f\\x87', 322: 'gor', 323: 'gorith', 324: 'of ', 325: 'char', 326: 'charac', 327: 'character', 328: ' a ', 329: 'al ', 330: 'le ', 331: 'ati', 332: '. Th', 333: 'tha', 334: 'that ', 335: 'comp', 336: 'res', 337: 'se ', 338: 'ï½', 339: 'â\\x80\\x8c', 340: 'â\\x80\\x8cð\\x9f\\x87', 341: 'ge ', 342: 'st', 343: ' al', 344: 'ent ', 345: 'tab', 346: ' an', 347: 'el', 348: 'ro', 349: 'with', 350: 'yte pair ', 351: 'es ', 352: 'oc', 353: 'for', 354: '] ', 355: 'lar', 356: 'mod', 357: ' l', 358: 'ini', 359: 'initi', 360: ' algorith', 361: 'ation', 362: 'igin', 363: 'and ', 364: 'ex', 365: ' be ', 366: 'ed in', 367: 'li', 368: 'le', 369: ' and ', 370: 'p ', 371: 'con', 372: 'e b', 373: ': ', 374: 'aa', 375: 'ur', 376: 'compres', 377: 'age ', 378: '.[', 379: 'ular', 380: 'if', 381: 'ne', 382: 'new', 383: 'to '}\n"
     ]
    }
   ],
   "source": [
    "tokens = global_tokens[:]\n",
    "vocab_size = 256 # because we used utf-8\n",
    "target_vocab_size = 384\n",
    "post_utf_map = run_bpe(tokens, vocab_size, target_vocab_size, early_stop_no_repeat=True)\n",
    "print(f\"Global tokens compressed from {len(global_tokens)} to {len(tokens)} tokens, compression ratio = {len(global_tokens)/len(tokens):.2f} \")\n",
    "print(f\"New vocab size = {vocab_size}, New tokens: \", post_utf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target vocab size vs compression ratio\n",
    "target_vocab_sizes = [vocab_size + i for i in range(1, 1024, 16)]\n",
    "compression_ratios = []\n",
    "for target_size in target_vocab_sizes:\n",
    "    tokens = global_tokens[:]\n",
    "    run_bpe(tokens, vocab_size, target_size, early_stop_no_repeat=True)\n",
    "    compression_ratios.append(len(global_tokens)/(1.0*len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#f7a79e",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "Trend",
         "type": "scatter",
         "x": [
          257,
          273,
          289,
          305,
          321,
          337,
          353,
          369,
          385,
          401,
          417,
          433,
          449,
          465,
          481,
          497,
          513,
          529,
          545,
          561,
          577,
          593,
          609,
          625,
          641,
          657,
          673,
          689,
          705,
          721,
          737,
          753,
          769,
          785,
          801,
          817,
          833,
          849,
          865,
          881,
          897,
          913,
          929,
          945,
          961,
          977,
          993,
          1009,
          1025,
          1041,
          1057,
          1073,
          1089,
          1105,
          1121,
          1137,
          1153,
          1169,
          1185,
          1201,
          1217,
          1233,
          1249,
          1265
         ],
         "y": [
          1.0300067430883344,
          1.2629185613890037,
          1.4150069476609541,
          1.531328320802005,
          1.6398282340311325,
          1.744717304397487,
          1.847037484885127,
          1.9421487603305785,
          2.0353097934710194,
          2.1259568545581073,
          2.213768115942029,
          2.2935435435435436,
          2.379283489096573,
          2.461724415793715,
          2.5268817204301075,
          2.595581988105353,
          2.668122270742358,
          2.7448337825696316,
          2.8260869565217392,
          2.9122974261201144,
          3.0039331366764994,
          3.1015228426395938,
          3.205666316894019,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027
         ]
        },
        {
         "marker": {
          "color": "#e74c3c",
          "size": 8
         },
         "mode": "markers",
         "name": "Data Points",
         "type": "scatter",
         "x": [
          257,
          273,
          289,
          305,
          321,
          337,
          353,
          369,
          385,
          401,
          417,
          433,
          449,
          465,
          481,
          497,
          513,
          529,
          545,
          561,
          577,
          593,
          609,
          625,
          641,
          657,
          673,
          689,
          705,
          721,
          737,
          753,
          769,
          785,
          801,
          817,
          833,
          849,
          865,
          881,
          897,
          913,
          929,
          945,
          961,
          977,
          993,
          1009,
          1025,
          1041,
          1057,
          1073,
          1089,
          1105,
          1121,
          1137,
          1153,
          1169,
          1185,
          1201,
          1217,
          1233,
          1249,
          1265
         ],
         "y": [
          1.0300067430883344,
          1.2629185613890037,
          1.4150069476609541,
          1.531328320802005,
          1.6398282340311325,
          1.744717304397487,
          1.847037484885127,
          1.9421487603305785,
          2.0353097934710194,
          2.1259568545581073,
          2.213768115942029,
          2.2935435435435436,
          2.379283489096573,
          2.461724415793715,
          2.5268817204301075,
          2.595581988105353,
          2.668122270742358,
          2.7448337825696316,
          2.8260869565217392,
          2.9122974261201144,
          3.0039331366764994,
          3.1015228426395938,
          3.205666316894019,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027,
          3.3027027027027027
         ]
        }
       ],
       "layout": {
        "height": 500,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "size": 12
         }
        },
        "hovermode": "x unified",
        "legend": {
         "x": 0.01,
         "xanchor": "left",
         "y": 0.99,
         "yanchor": "top"
        },
        "margin": {
         "b": 60,
         "l": 80,
         "r": 40,
         "t": 80
        },
        "paper_bgcolor": "#f8f8f8",
        "plot_bgcolor": "#f8f8f8",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "#333333",
          "size": 20
         },
         "text": "Compression Ratio of BPE Algorithm",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 700,
        "xaxis": {
         "gridcolor": "lightgrey",
         "showgrid": true,
         "tickfont": {
          "size": 12
         },
         "title": {
          "font": {
           "color": "#333333",
           "size": 14
          },
          "text": "New Vocabulary Size"
         },
         "zeroline": false
        },
        "yaxis": {
         "gridcolor": "lightgrey",
         "showgrid": true,
         "tickfont": {
          "size": 12
         },
         "title": {
          "font": {
           "color": "#333333",
           "size": 14
          },
          "text": "Compression Ratio<br>(Original / merged length)"
         },
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot generated with Claude, an attempt to get familiar with plotly instead of using matplotlib.\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure with a specific width and height\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define colors\n",
    "point_color = '#e74c3c'  # Original dark red color for data points\n",
    "line_color = '#f7a79e'   # Lighter version of the red for the line\n",
    "\n",
    "# Add the dotted line plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=target_vocab_sizes,\n",
    "    y=compression_ratios,\n",
    "    mode='lines',\n",
    "    line=dict(color=line_color, width=2, dash='dot'),\n",
    "    name='Trend'\n",
    "))\n",
    "\n",
    "# Add markers at data points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=target_vocab_sizes,\n",
    "    y=compression_ratios,\n",
    "    mode='markers',\n",
    "    marker=dict(color=point_color, size=8),\n",
    "    name='Data Points'\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'Compression Ratio of BPE Algorithm',\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top',\n",
    "        'font': dict(size=20, color='#333333')\n",
    "    },\n",
    "    xaxis_title={\n",
    "        'text': \"New Vocabulary Size\",\n",
    "        'font': dict(size=14, color='#333333')\n",
    "    },\n",
    "    yaxis_title={\n",
    "        'text': \"Compression Ratio<br>(Original / merged length)\",\n",
    "        'font': dict(size=14, color='#333333')\n",
    "    },\n",
    "    plot_bgcolor='#f8f8f8',\n",
    "    paper_bgcolor='#f8f8f8',\n",
    "    hovermode='x unified',\n",
    "    hoverlabel=dict(bgcolor=\"white\", font_size=12),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgrey',\n",
    "        tickfont=dict(size=12),\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor='lightgrey',\n",
    "        tickfont=dict(size=12),\n",
    "        zeroline=False\n",
    "    ),\n",
    "    width=700,  # Set the width of the plot\n",
    "    height=500,  # Set the height of the plot\n",
    "    margin=dict(l=80, r=40, t=80, b=60),  # Adjust margins\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global tokens compressed from 3055 to 1505 tokens, compression ratio = 2.03 \n",
      "New vocab size = 256, New tokens:  {256: 'e ', 257: 's ', 258: 'th', 259: 'in', 260: 'en', 261: 't ', 262: 'd ', 263: 're', 264: 'er', 265: 'the ', 266: ' a', 267: 'co', 268: ', ', 269: 'ac', 270: 'or', 271: 'is ', 272: 'ing', 273: 'ed ', 274: 'ta', 275: 'al', 276: ' b', 277: '. ', 278: 'to', 279: 'ti', 280: 'on', 281: 'ar', 282: ' the ', 283: 'ð\\x9f', 284: 'an', 285: 'ith', 286: 'ir', 287: 'of', 288: 'pl', 289: 'ter', 290: 'Th', 291: 'y ', 292: 'ab', 293: 'pa', 294: 'yt', 295: 'plac', 296: 'cod', 297: 'pair', 298: 'ce', 299: 'ken', 300: 'm ', 301: 'qu', 302: 'mo', 303: 'ch', 304: 'ig', 305: 'ing ', 306: 'encod', 307: 'no', 308: 'com', 309: 'da', 310: 'replac', 311: 'token', 312: 'un', 313: 'se', 314: 'st ', 315: 'yte ', 316: 'pair ', 317: 'data', 318: 'si', 319: 'ð\\x9f\\x85', 320: 'â\\x80', 321: 'ð\\x9f\\x87', 322: 'gor', 323: 'gorith', 324: 'of ', 325: 'char', 326: 'charac', 327: 'character', 328: ' a ', 329: 'al ', 330: 'le ', 331: 'ati', 332: '. Th', 333: 'tha', 334: 'that ', 335: 'comp', 336: 'res', 337: 'se ', 338: 'ï½', 339: 'â\\x80\\x8c', 340: 'â\\x80\\x8cð\\x9f\\x87', 341: 'ge ', 342: 'st', 343: ' al', 344: 'ent ', 345: 'tab', 346: ' an', 347: 'el', 348: 'ro', 349: 'with', 350: 'yte pair ', 351: 'es ', 352: 'oc', 353: 'for', 354: '] ', 355: 'lar', 356: 'mod', 357: ' l', 358: 'ini', 359: 'initi', 360: ' algorith', 361: 'ation', 362: 'igin', 363: 'and ', 364: 'ex', 365: ' be ', 366: 'ed in', 367: 'li', 368: 'le', 369: ' and ', 370: 'p ', 371: 'con', 372: 'e b', 373: ': ', 374: 'aa', 375: 'ur', 376: 'compres', 377: 'age ', 378: '.[', 379: 'ular', 380: 'if', 381: 'ne', 382: 'new', 383: 'to '}\n"
     ]
    }
   ],
   "source": [
    "tokens = global_tokens[:]\n",
    "vocab_size = 256 # because we used utf-8\n",
    "target_vocab_size = 384\n",
    "post_utf_map = run_bpe(tokens, vocab_size, target_vocab_size, early_stop_no_repeat=True)\n",
    "print(f\"Global tokens compressed from {len(global_tokens)} to {len(tokens)} tokens, compression ratio = {len(global_tokens)/len(tokens):.2f} \")\n",
    "print(f\"New vocab size = {vocab_size}, New tokens: \", post_utf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, token_map):\n",
    "    \"\"\"Given token ids, return the raw text\"\"\"\n",
    "    token_map.update({i: chr(i) for i in range(256)})\n",
    "    text = [\"\"] * len(ids)\n",
    "    for i, id in enumerate(ids):\n",
    "        text[i] = token_map[id]\n",
    "    return \"\".join(text)\n",
    "\n",
    "def encode(text, token_map):\n",
    "    \"\"\"Given raw text, return the token ids\"\"\"\n",
    "    # find out longest token so we can start search here.\n",
    "    len_longest_token = 1\n",
    "    for t in token_map.values():\n",
    "        if len(t) > len_longest_token:\n",
    "            len_longest_token = len(t)\n",
    "\n",
    "    inverse_token_map = {v:k for k,v in token_map.items()}\n",
    "    inverse_token_map.update({chr(i): i for i in range(256)})\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        # search for tokens of length of the longest token and keep\n",
    "        # reducing the substring length to search until a matching token is found\n",
    "        search_substr_end = min(i + len_longest_token, len(text)) \n",
    "        token_added = False\n",
    "        while search_substr_end > i:\n",
    "            if text[i:search_substr_end] in inverse_token_map:\n",
    "                tokens.append(inverse_token_map[text[i:search_substr_end]])\n",
    "                token_added = True\n",
    "                i = search_substr_end\n",
    "                break                \n",
    "            else:\n",
    "                search_substr_end -= 1\n",
    "        assert token_added, f\"Unknown token {text[i]} found at index {i}, unexpected!\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING decode...\n",
      "test case 0: [] -> {} -> \n",
      "\tPASS\n",
      "test case 1: [65, 66, 67] -> {} -> ABC\n",
      "\tPASS\n",
      "test case 2: [256, 65] -> {256: 'Hello '} -> Hello A\n",
      "\tPASS\n",
      "test case 3: [256, 257, 258] -> {256: 'Hello', 257: ' world', 258: '!'} -> Hello world!\n",
      "\tPASS\n",
      "test case 4: [256, 32, 257] -> {256: 'Hello', 257: 'world!'} -> Hello world!\n",
      "\tPASS\n",
      "test case 5: [256, 257, 67] -> {256: 'こん', 257: 'にちは'} -> こんにちはC\n",
      "\tPASS\n",
      "test case 6: [256, 257, 258] -> {256: 'a', 257: 'b', 258: 'c'} -> abc\n",
      "\tPASS\n",
      "test case 7: [65, 256, 66, 257] -> {256: 'BC', 257: 'DE'} -> ABCBDE\n",
      "\tPASS\n",
      "test case 8: [256, 257, 256] -> {256: 'repeat', 257: ' '} -> repeat repeat\n",
      "\tPASS\n",
      "test case 9: [256, 257, 258, 259] -> {256: 'a', 257: 'b', 258: 'c', 259: 'defg'} -> abcdefg\n",
      "\tPASS\n",
      "test case 10: [256, 65, 257, 66] -> {256: 'Hello', 257: 'World'} -> HelloAWorldB\n",
      "\tPASS\n",
      "-------------------------\n",
      "11 PASS, 0 FAIL.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_decode(ids, token_map, expected_text):\n",
    "    text = decode(ids, token_map)\n",
    "    assert text == expected_text, f\"Expected {expected_text}, got {text}.\"\n",
    "\n",
    "def test_encode(text, token_map, expected_ids):\n",
    "    ids = encode(text, token_map)\n",
    "    assert ids == expected_ids, f\"Expected {expected_ids}, got {ids}\"\n",
    "\n",
    "test_cases_decode = [\n",
    "    ([], {}, \"\"),  # Empty input\n",
    "    ([65, 66, 67], {}, \"ABC\"),  # Basic ASCII characters\n",
    "    ([256, 65], {256: \"Hello \"}, \"Hello A\"),  # Mixed token and ASCII\n",
    "    ([256, 257, 258], {256: \"Hello\", 257: \" world\", 258: \"!\"}, \"Hello world!\"),  # All tokens\n",
    "    ([256, 32, 257], {256: \"Hello\", 257: \"world!\"}, \"Hello world!\"),  # Tokens with space\n",
    "    ([256, 257, 67], {256: \"こん\", 257: \"にちは\"}, \"こんにちはC\"),  # Non-ASCII characters\n",
    "    ([256, 257, 258], {256: \"a\", 257: \"b\", 258: \"c\"}, \"abc\"),  # Single-char tokens\n",
    "    ([65, 256, 66, 257], {256: \"BC\", 257: \"DE\"}, \"ABCBDE\"),  # Interleaved ASCII and tokens\n",
    "    ([256, 257, 256], {256: \"repeat\", 257: \" \"}, \"repeat repeat\"),  # Repeated tokens\n",
    "    ([256, 257, 258, 259], {256: \"a\", 257: \"b\", 258: \"c\", 259: \"defg\"}, \"abcdefg\"),  # Mixed length tokens\n",
    "    ([256, 65, 257, 66], {256: \"Hello\", 257: \"World\"}, \"HelloAWorldB\")  # Tokens with interspersed ASCII\n",
    "]\n",
    "print(\"\\nTESTING decode...\")\n",
    "fail_count = 0\n",
    "for i, t_d in enumerate(test_cases_decode):\n",
    "    print(f\"test case {i}: {t_d[0]} -> {t_d[1]} -> {t_d[2]}\")\n",
    "    try:\n",
    "        test_decode(*t_d)\n",
    "        print(\"\\tPASS\")\n",
    "    except Exception as e:\n",
    "        fail_count += 1\n",
    "        print(f\"...... FAIL. {str(e)}\")\n",
    "print(\"-------------------------\")\n",
    "print(f\"{len(test_cases_decode) - fail_count} PASS, {fail_count} FAIL.\")\n",
    "print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING encode...\n",
      "test case 0:  -> {} -> []\n",
      "\tPASS\n",
      "test case 1: ABC -> {} -> [65, 66, 67]\n",
      "\tPASS\n",
      "test case 2: Hello A -> {256: 'Hello '} -> [256, 65]\n",
      "\tPASS\n",
      "test case 3: Hello world! -> {256: 'Hello', 257: ' world'} -> [256, 257, 33]\n",
      "\tPASS\n",
      "test case 4: Hello world! -> {256: 'Hello', 257: 'world!'} -> [256, 32, 257]\n",
      "\tPASS\n",
      "test case 5: こんにちはC -> {256: 'こん', 257: 'にちは'} -> [256, 257, 67]\n",
      "\tPASS\n",
      "test case 6: abc -> {256: 'ab', 257: 'bc', 258: 'ca'} -> [256, 99]\n",
      "\tPASS\n",
      "test case 7: ABCBDE -> {256: 'BC', 257: 'DE'} -> [65, 256, 66, 257]\n",
      "\tPASS\n",
      "test case 8: repeat repeat -> {256: 'repeat'} -> [256, 32, 256]\n",
      "\tPASS\n",
      "test case 9: abcdefg -> {256: 'ab', 259: 'defg'} -> [256, 99, 259]\n",
      "\tPASS\n",
      "test case 10: HelloAWorldB -> {256: 'Hello', 257: 'World'} -> [256, 65, 257, 66]\n",
      "\tPASS\n",
      "test case 11: TestNoMatch -> {256: 'Other'} -> [84, 101, 115, 116, 78, 111, 77, 97, 116, 99, 104]\n",
      "\tPASS\n",
      "test case 12: ABCabc -> {256: 'ABC', 257: 'abc'} -> [256, 257]\n",
      "\tPASS\n",
      "test case 13: ABCABCABC -> {256: 'ABC'} -> [256, 256, 256]\n",
      "\tPASS\n",
      "test case 14: A B C -> {256: 'A B C', 257: 'A B'} -> [256]\n",
      "\tPASS\n",
      "-------------------------\n",
      "15 PASS, 0 FAIL.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "test_cases_encode = [\n",
    "    (\"\", {}, []),  # Empty input\n",
    "    (\"ABC\", {}, [65, 66, 67]),  # Basic ASCII characters\n",
    "    (\"Hello A\", {256: \"Hello \"}, [256, 65]),  # Mixed token and ASCII\n",
    "    (\"Hello world!\", {256: \"Hello\", 257: \" world\"}, [256, 257, 33]),  # All tokens\n",
    "    (\"Hello world!\", {256: \"Hello\", 257: \"world!\"}, [256, 32, 257]),  # Tokens with space\n",
    "    (\"こんにちはC\", {256: \"こん\", 257: \"にちは\"}, [256, 257, 67]),  # Non-ASCII characters\n",
    "    (\"abc\", {256: \"ab\", 257: \"bc\", 258: \"ca\"}, [256, 99]),  # Single-char tokens\n",
    "    (\"ABCBDE\", {256: \"BC\", 257: \"DE\"}, [65, 256, 66, 257]),  # Interleaved ASCII and tokens\n",
    "    (\"repeat repeat\", {256: \"repeat\"}, [256, 32, 256]),  # Repeated tokens\n",
    "    (\"abcdefg\", {256: \"ab\", 259: \"defg\"}, [256, 99, 259]),  # Mixed length tokens\n",
    "    (\"HelloAWorldB\", {256: \"Hello\", 257: \"World\"}, [256, 65, 257, 66]),  # Tokens with interspersed ASCII\n",
    "    (\"TestNoMatch\", {256: \"Other\"}, [84, 101, 115, 116, 78, 111, 77, 97, 116, 99, 104]),  # No matching tokens\n",
    "    (\"ABCabc\", {256: \"ABC\", 257: \"abc\"}, [256, 257]),  # Case-sensitive tokens\n",
    "    (\"ABCABCABC\", {256: \"ABC\"}, [256, 256, 256]),  # Repeated token\n",
    "    (\"A B C\", {256: \"A B C\", 257: \"A B\"}, [256])  # Longest match priority\n",
    "]\n",
    "\n",
    "print(\"\\nTESTING encode...\")\n",
    "fail_count = 0\n",
    "for i, t_e in enumerate(test_cases_encode):\n",
    "    print(f\"test case {i}: {t_e[0]} -> {t_e[1]} -> {t_e[2]}\")\n",
    "    try:\n",
    "        test_encode(*t_e)\n",
    "        print(\"\\tPASS\")\n",
    "    except Exception as e:\n",
    "        fail_count += 1\n",
    "        print(f\"xxxxxx FAIL. {str(e)}\")\n",
    "print(\"-------------------------\")\n",
    "print(f\"{len(test_cases_encode) - fail_count} PASS, {fail_count} FAIL.\")\n",
    "print(\"-------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Unknown token Ｕ found at index 0, unexpected!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_utf_map\u001b[49m\u001b[43m)\u001b[49m, post_utf_map) \u001b[38;5;241m==\u001b[39m global_text)\n",
      "Cell \u001b[0;32mIn[180], line 34\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(text, token_map)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m             search_substr_end \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m token_added, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, unexpected!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unknown token Ｕ found at index 0, unexpected!"
     ]
    }
   ],
   "source": [
    "print(decode(encode(global_text, post_utf_map), post_utf_map) == global_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼µï½ï½ï½ï½ï½ï½! ð¤ððððððâ½ ðºâð³âð®âð¨âð´âð©âðª! ð ËÃÂµÅÏÅâ«Â¥Â¨Â©âÃ¸ ^ _ . Âº ' ` Ë Ã¿ Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial 'tokens'). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] This algorithmic approach have been extended from spoken language to sign language in recent years.[9]All the unique tokens found in a corpus are listed in a token vocabulary, the size of which, in the case of GPT-3.5 and GPT-4, is 100256.. The difference between the modified and the original algorithm is that the original algorithm does not merge the most frequent pair of bytes of data, but replaces them by a new byte that was not contained in the initial dataset. A lookup table of the replacements is required to rebuild the initial dataset. The algorithm is effective for tokenization because it has low computational overhead and remains consistent and reliable. Original algorithm: The original algorithm operates by iteratively replacing the most common contiguous sequences of characters in a target text with unused 'placeholder' bytes. The iteration ends when no sequences can be found, leaving the target text effectively compressed. Decompression can be performed by reversing this process, querying known placeholder terms against their corresponding denoted sequence, using a lookup table. In the original paper, this lookup table is encoded and stored alongside the compressed text. Example: Suppose the data to be encoded is aaabdaaabac The byte pair 'aa' occurs most often, so it will be replaced by a byte that is not used in the data, such as 'Z'. Now there is the following data and replacement table: ZabdZabac, Z=aa, Then the process is repeated with byte pair 'ab', replacing it with 'Y': ZYdZYac, Y=ab, Z=aa. The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing 'ZY' with 'X': XdXac, X=ZY, Y=ab, Z=aa. This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once. To decompress the data, simply perform the replacements in the reverse order.\n"
     ]
    }
   ],
   "source": [
    "print(decode(tokens, post_utf_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differences in Andrej Karpathy's *naive* implementation**\n",
    "* Same split of helper functions: get_most_common and merge\n",
    "* In get most common: he builds a dictionary of counts, and uses python's max() function with the count key.\n",
    "* The merge logic is effectively the same, he uses one pointer, I use two pointers which I found more intuitive.\n",
    "* The iterative loop is the same too, except he does a for loop.\n",
    "* For encoding and decoding, he works in bytes, I work directly on strings. The prod models work in bytes, so when the llm outputs a non-valid utf-8 byte sequence, we can replace it with an error character. Working in strings means we can decode erroneously for the special chars whose representations are multiple bytes. We can see that error when we encode and decode the entire global string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Tokenizers\n",
    "* WordPieceEncoding: used in BERT and Electra. Takes all alphabets from input data to make sure nothing will be UNK (unknown), also tries to for tokens based on their frequency.\n",
    "* SentencePieceEncoding: Others assume white-space is the defacto word separated, which may not always be true. This addresses that issue? \n",
    "* Maximum Prefix Encoding: ?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
